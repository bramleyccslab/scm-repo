---
# Documentation: https://wowchemy.com/docs/managing-content/

title: "Inductive Reasoning"
summary: " "
authors: []
tags: [sem1, w5, r]
categories: []
date: 2023-07-01T09:58:49+01:00
weight: 6
# Optional external URL for project (replaces project detail page).
external_link: ""

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ""
  focal_point: ""
  preview_only: false

# Custom links (optional).
#   Uncomment and edit lines below to show custom links.
# links:
# - name: Follow
#   url: https://twitter.com
#   icon_pack: fab
#   icon: twitter

url_code: ""
url_pdf: ""
url_slides: ""
url_video: ""

# Slides (optional).
#   Associate this project with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides = "example-slides"` references `content/slides/example-slides.md`.
#   Otherwise, set `slides = ""`.
slides: ""
---

While Sherlock's over there blogging *The Science of Deduction*, I'd argue most of human reasoning is **inductive**. We see lot's of examples (e.g., 10 million white swans) and then try to explain them (swans are white). Checks out, right?

## Primary Readings

Everyone should read these and be prepared to discuss:

|  |  |
|:----:|:-----|
| {{< icon name="scroll" pack="fas" >}} Goodman, N. (1955) | {{< details "The Riddle of Induction. Chapter 3 of *Fact, fiction, and forecast* (pp. 59--83). Cambridge, MA: Harvard University Press.">}}
{{< icon name="robot" pack="fas" >}} TODO 
{{< icon name="chalkboard-teacher" pack="fas" >}} I couldn't find a machine readable pdf of this one so no gpt summary.

This is one of the foundational philosophical papers in cognitive science, and Nelson Goodman (not to be confused with the living scientist Noah Goodman below) was a highly influential philosopher of science. The thought experiment about Grue, in particular, has been a lasting reductio ad absurdum for the project of formally grounding inductive knowledge. {{</details>}} |
| {{< icon name="scroll" pack="fas" >}} Tenenbaum, J. B., Kemp, C., Griffiths, T. L., & Goodman, N. D. (2011). | {{< details "How to grow a mind: Statistics, structure, and abstraction. *Science*, 331(6022), 1279-1285.">}}
{{< icon name="robot" pack="fas" >}}  The article explores how the mind develops through the use of statistics, structure, and abstraction. It emphasizes the importance of understanding how the mind processes information and how it can be applied to various fields such as artificial intelligence and cognitive science. It focuses on the challenges of learning from sparse, noisy, and ambiguous data and highlights the ability of children to learn new words and concepts from just a few examples. It explains how Bayesian principles in the human mind are applied to specific cognitive capacities and modules. The article discusses the importance of abstract knowledge in learning, how learners acquire this knowledge, and different forms of abstract knowledge representation used in Bayesian cognitive models. It also discusses the concept of hierarchical Bayesian models and their role in learning abstract knowledge. The article concludes by highlighting the potential of Bayesian approaches in understanding cognition and its origins.

{{< icon name="chalkboard-teacher" pack="fas" >}} Here's another relatively modern paper as a comparison. Again it is interesting to compare the writing styles. It would seem like hierarchical Bayesian inference can help make more sense of how our inductive reasoning works, but does it really solve the riddle of induction? Note this paper is also quite relevant to the Rationality topic. {{</details>}} |

## Secondary Readings

The presenter should read and incorporate these:

|  |  |
|:----:|:-----|
| {{< icon name="scroll" pack="fas" >}} Goyal, A., & Bengio, Y. (2022). | {{< details "Inductive biases for deep learning of higher-level cognition. *Proceedings of the Royal Society A*, 478 (20210068).">}}
{{< icon name="robot" pack="fas" >}}  This paper examines the hypothesis that human and animal intelligence can be explained by a few principles. It focuses on the inductive biases used in deep learning for higher-level and sequential conscious processing in order to close the gap between current deep learning and human cognitive abilities. It emphasizes the need for additional inductive biases to achieve flexibility, robustness, and adaptability in deep learning models, particularly through knowledge representation. It also discusses the limitations of current machine learning systems in terms of their performance, generalizability, and robustness.

{{< icon name="chalkboard-teacher" pack="fas" >}} This paper is written by some of the most well known researchers in machine learning. It highlights very similar issues and solutions to problems of inductive generalization and the role of priors or inductive biases as arise in the human cognition literature.{{</details>}}|

## Questions under discussion

- How/do humans rationally solve the problem of induction?
- What are inductive biases and what are they good for?

